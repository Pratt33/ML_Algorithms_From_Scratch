{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMECd5mCkDsd5m5OlQTeA9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pratt33/ML_Algorithms_From_Scratch/blob/main/Ridge_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Objective\n",
        "\n",
        "The goal is to predict the progression of diabetes in patients based on various medical features. Accurate predictions help healthcare providers make informed decisions regarding treatment plans, resource allocation, and early intervention strategies."
      ],
      "metadata": {
        "id": "Pb91uj7jqwl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "Given a dataset of patient features (such as BMI, blood pressure, and serum measurements), we aim to build a Ridge Regression model to predict diabetes progression. This model will be trained using gradient descent with L2 regularization to prevent overfitting and improve generalization."
      ],
      "metadata": {
        "id": "dvi3IpljqzLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Constraints\n",
        "\n",
        "- Accuracy & Interpretability – Reliable predictions while remaining understandable for medical use.\n",
        "\n",
        "- Efficiency & Scalability – Optimized training, low-cost deployment, and seamless integration.\n",
        "\n",
        "- Generalization & Robustness – Prevent overfitting and adapt to new data trends.\n",
        "\n",
        "- Compliance & Data Quality – Ensure privacy (HIPAA, GDPR) and handle missing/inconsistent data."
      ],
      "metadata": {
        "id": "0opWl6iZq2oL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OtFPlazaiYzg"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)"
      ],
      "metadata": {
        "id": "5nMoamxRpFj1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
      ],
      "metadata": {
        "id": "rzFwBW3upFmi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RidgeGradientDescent:\n",
        "    def __init__(self, epochs=1000, learning_rate=0.05, alpha=0.01):\n",
        "        \"\"\"\n",
        "        Ridge Regression using Gradient Descent.\n",
        "        :param epochs: Number of training iterations.\n",
        "        :param learning_rate: Step size for updates.\n",
        "        :param alpha: Regularization strength (L2 penalty).\n",
        "        \"\"\"\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.alpha = alpha\n",
        "        self.coef_ = None  # Model weights\n",
        "        self.intercept_ = None  # Bias term\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Train the model using Ridge Regression with Gradient Descent.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X_train.shape\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.coef_ = np.zeros(n_features)\n",
        "        self.intercept_ = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            # Compute predictions\n",
        "            y_pred = np.dot(X_train, self.coef_) + self.intercept_\n",
        "\n",
        "            # Compute gradients\n",
        "            intercept_gradient = -2 * np.mean(y_train - y_pred)  # Bias gradient\n",
        "            coef_gradient = (-2 / n_samples) * np.dot(X_train.T, (y_train - y_pred))  # Weight gradient\n",
        "\n",
        "            # Apply Ridge penalty (L2 regularization)\n",
        "            coef_gradient += self.alpha * self.coef_\n",
        "\n",
        "            # Update parameters\n",
        "            self.intercept_ -= self.lr * intercept_gradient\n",
        "            self.coef_ -= self.lr * coef_gradient\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        \"\"\"\n",
        "        Predict target values for new data.\n",
        "        \"\"\"\n",
        "        return np.dot(X_test, self.coef_) + self.intercept_"
      ],
      "metadata": {
        "id": "zw0Sw8hIpFpD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train Ridge Regression model\n",
        "reg = RidgeGradientDescent(epochs=1000, learning_rate=0.005, alpha=0.001)\n",
        "reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "c6YDLjgSpFrE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "A2K_KJpspFtT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance\n",
        "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
        "print(\"Coefficients:\", reg.coef_)\n",
        "print(\"Intercept:\", reg.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeawHTt7pFwD",
        "outputId": "525d4f52-60e7-4d32-b432-6f9302a77ac2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R² Score: 0.04878328746697225\n",
            "Coefficients: [  7.9433393    1.49164596  20.82186886  16.32179092   7.37627873\n",
            "   5.61952986 -13.1727559   13.84145196  20.80222634  12.62582195]\n",
            "Intercept: 150.59253661759246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "- The Ridge Regression model was implemented using gradient descent with L2 regularization.\n",
        "\n",
        "- The improved code ensures better gradient updates and avoids issues with scaling.\n",
        "\n",
        "- R² score indicates the model's effectiveness in capturing variance in the data.\n",
        "\n",
        "- Regularization prevents overfitting, making the model more generalizable."
      ],
      "metadata": {
        "id": "Fsdwml8VrYJH"
      }
    }
  ]
}